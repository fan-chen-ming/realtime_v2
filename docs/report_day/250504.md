项目讲义

1.[Test01_FlinkCDC.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdim%2FTest01_FlinkCDC.java)

前端埋点数据,拿到数据 从flume 采集 到kfk  发送到下游 flink 日志
业务数据是cdc 像 mysql 的binlog 去拿数据
拿到数据 之后 设置 checkpoint 将数据源打包成数据流

useSSL=false	禁用 SSL 连接	提高性能，测试环境可用
decimal.handling.mode=double	将 decimal 转为 double	避免 JSON 序列化失败
time.precision.mode=connect	使用 connect 模式时间戳	兼容 Kafka Connect 时间格式
chunk.key-column=id	    分片主键	控制快照分片粒度，提高并发

CheckPoint + StateBackend：保障作业失败重启时的状态恢复。
Kafka DeliveryGuarantee.AT_LEAST_ONCE：至少一次投递语义，防止数据丢失。
CDC Source 自动重连机制：MySQL 断开连接后自动恢复，不中断作业。


问题与解决方案（常见答辩提问）
Q1: 如何保证数据一致性？
使用 Checkpoint + Kafka Sink 的 AT_LEAST_ONCE 语义，结合幂等消费者可以做到最终一致。
Q2: 如果 MySQL 表结构发生变化怎么办？
CDC 会自动感知 DDL 变化，并在事件中携带 schema 信息，下游需支持 schema evolution（如使用 Avro）。
Q3: 如何处理大字段或者二进制字段？
示例中的 price 字段是 base64 编码的 binary 类型，建议提前确认字段类型，必要时进行解码处理。
Q4: 并行度设置多少合适？为什么？
一般根据数据量大小和 Kafka 分区数决定。本例设为 4，适配中小规模数据集。